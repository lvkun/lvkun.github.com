## 读书笔记 《数学之美》（一）

作者：吴军

从书中节选几章感兴趣的做了笔记。

### 统计语言模型

贾里尼克（ Frederek Jelinek ）用简单的统计模型判断文字序列是否合乎文法、含义是否正确的问题：
一个句子是否合理，就看看它的可能性大小如何。

假设 $S$ 表示某一个有意义的句子，由一连串特定顺序排列的词
$w\_1,w\_2,...,w\_n$ 组成，这里 $n$ 是句子的长度。$S$ 的概率 $P(S)$ ：

$P(S) = P(w\_1,w\_2,...,w\_n) = P(w\_1) \cdot P(w\_2|w\_1) \cdot P(w\_3|w\_1,w\_2) \cdots P(w\_n|w\_1,w\_2,...,w\_{n-1})$

引入 [马尔可夫假设] 可以简化运算，即假设任意一个词 $w\_i$ 出现的概率只同它前面的词 $w\_{i-1}$ 有关：

$P(S) = P(w\_1) \cdot P(w\_2|w\_1) \cdot P(w\_2|w\_1) \cdot P(w\_3|w\_4) \cdots P(w\_i|w\_{i-1}) \cdots P(w\_n|w\_{n-1}))$

该公式对应的统计语言模型是二元模型（Bigram Model）。也可以假设一个词由前面 $N - 1$ 个词决定，被称为 $N$ 元模型。

#### 计算条件概率

$P(w\_i|w\_{i-1}) = \frac{P(w\_{i-1}, w\_i)}{P(w\_{i-1})}$

计算 $w\_{i-1}, w\_i$ 这对词在统计的文本中前后相邻出现了多少次 $\\#(w\_{i-1}, w\_i)$ ，以及 $w\_{i-1}$ 本身在同样的文本中出现了多少次 $\\#(w\_{i-1})$ ，然后用两个数分别除以语料库的大小 $\\#$ ，即可得到这些词或二元组的相对频度：

$f(w\_{i-1}, w\_i) = \frac{\\#(w\_{i-1}, w\_i)}{\\#}$

$f(w\_{i-1}) = \frac{\\#(w\_{i-1})}{\\#}$

根据大数定理，只要统计量足够，相对频度就等于概率，再考虑到分母相同：

$P(w\_i|w\_{i-1}) \approx \frac{\\#(w\_{i-1}, w\_i)}{\\#(w\_{i-1})}$

#### 高阶语言模型

假定文本中的每个词 $w\_i$ 和前面 $N-1$ 个词有关，而与更前面的词无关，这样当前词 $w\_i$ 的概率只取决于前面 $N-1$ 个词 $P(w\_{i-N+1}, w\_{i-N+2}, ..., w\_{i-1})$

$P(w\_i|w\_1, w\_2, ..., w\_{i-1}) = P(w\_i|w\_{i-N+1}, w\_{i-N+2}, ..., w\_{i-1})$

这种假设被称为 $N-1$ 阶马尔可夫假设，对应的语言模型称为 $N$ 元模型（N-Gram Model）。

为什么 $N$ 一般取值都这么小？

$N$ 元模型的空间复杂度（$O(|V|^N)$）和时间复杂度（$O(|V|^{N-1})$）是指数函数。当模型从3到4时，效果的提升就不是很显著，而资源的耗费增加非常快。

自然语言中，上下文之间的相关性可能跨度非常大，即使模型的阶数再提高，也无可奈何，这时就要采用其他一些长程的依赖性（Long Distance Dependency）来解决这个问题。

#### 模型的训练、零概率问题和平滑方法

1953年古德（I. J. Good）在他老板图灵（Alan Turing）的指导下，提出了在统计中相信可靠的统计数据，而对不可信的统计数据打折扣的一种概率估计方法，还给出了一个一个很漂亮的重新估算概率的公式，这个公式后来被称为古德图灵估计（Good-Turing Estimate）。

假定在语料库中出现 $r$ 次的词有 $N\_r$ 个， 特别地，未出现的词数量为 $N\_0$ 。语料库的大小为 $N$ 。

$N = {\displaystyle\sum\_{i=1}^{\infty}{rN\_r}}$

出现 $r$ 次的词在整个语料库中的相对频度（Relative Frequency）则是 $r/N$ 。

出现 $r$ 次的词在计算它们的概率时使用一个更小的次数 $d\_r$ ：

$d\_r = (r+1) \cdot N\_{r+1}/N\_r$

显然：

$\displaystyle\sum\_r{d\_r \cdot N\_r} = N$

这样出现 $r$ 次的词出现概率估计为 $d\_r/N$。

估计二元模型概率如下：

$P(w\_i|w\_{i-1}) = \begin{cases}
 f(w\_i|w\_{i-1}) & \text  { if } \\#(w\_{i-1}, w\_i) \geq T \\\\ 
 f\_{gt}(w\_i|w\_{i-1}) & \text  { if } 0 \\lt \\#(w\_{i-1}, w\_i) \\lt T \\\\ 
 Q(w\_{i-1}) \cdot f(w\_i) & \text {otherwise} 
\end{cases}$

其中 $T$ 是一个阈值，一般在 $8 - 10$ 左右。函数 $f\_{gt}()$ 表示经过古德 - 图灵估计后的相对频度，而

$Q(w\_{i-1}) = \frac{1 - \displaystyle\sum\_{w\_iseen}{P(w\_i|w\_{i-1})}}{\displaystyle\sum_{w\_iunseen}{f(w\_i)}}$

类似地，对于三元模型，概率估计如下：

$P(w\_i|w\_{i-2}, w\_{i-1}) = \begin{cases}
 f(w\_i|w\_{i-2}, w\_{i-1}) & \text  { if } \\#(w\_{i-2}, w\_{i-1}, w\_i) \geq T \\\\ 
 f\_{gt}(w\_i|w\_{i-2}, w\_{i-1}) & \text  { if } 0 \\lt \\#(w\_{i-2}, w\_{i-1}, w\_i) \\lt T \\\\ 
 Q(w\_{i-2}, w\_{i-1}) \cdot P(w\_i|w\_{i-1}) & \text {otherwise} 
\end{cases}$

#### 语料的选取问题

* 语料要与模型的应用领域相关
* 训练数据通常是越多越好
* 训练语料的噪声高低也会对模型的效果产生一定影响

[马尔可夫假设]: http://en.wikipedia.org/wiki/Markov_chain

### 中文分词

利用统计语言模型进行分词。假设一个句子 $S$ 可以有几种分词方法：

$\begin{cases}
A\_1,A\_2,A\_3,\cdots,A\_k \\\\
B\_1,B\_2,B\_3,\cdots,B\_m \\\\
C\_1,C\_2,C\_3,\cdots,C\_n \\\\
\end{cases}$

如果 $A\_1,A\_2,A\_3,\cdots,A\_k$ 是最好的分法，那么其概率满足

$P(A\_1,A\_2,A\_3,\cdots,A\_k) > P(B\_1,B\_2,B\_3,\cdots,B\_m)$

并且

$P(A\_1,A\_2,A\_3,\cdots,A\_k) > P(C\_1,C\_2,C\_3,\cdots,C\_n)$

利用统计语言模型计算出每种分词后句子出现的概率，并找出其中概率最大的，就能够找到最好的分词方法

### 隐含马尔可夫模型

在通信中，如何根据接收端的观测信号 $o\_1,o\_2,o\_3,\cdots$ 来推测信号源发送的信息 $s\_1,s\_2,s\_3,\cdots$ 呢？只需要从所有的源信息中找到最可能产生出观测信号的那个信息。用概率论的语言来描述，就是在已知 $o\_1,o\_2,o\_3,\cdots$ 的情况下，求得令条件概率 $P(s\_1,s\_2,s\_3,\cdots|o\_1,o\_2,o\_3,\cdots)$ 达到最大值的那个信息串 $s\_1,s\_2,s\_3,\cdots$ 。

而 $P(s\_1,s\_2,s\_3,\cdots|o\_1,o\_2,o\_3,\cdots)$ 利用贝叶斯公式可以等价变换成：

$\frac{P(o\_1,o\_2,o\_3,\cdots|s\_1,s\_2,s\_3,\cdots) \cdot P(s\_1,s\_2,s\_3,\cdots)}{P(o\_1,o\_2,o\_3,\cdots)}$

$P(o\_1,o\_2,o\_3,\cdots)$ 是可以忽略的常数。上述公式等价于：

$P(o\_1,o\_2,o\_3,\cdots|s\_1,s\_2,s\_3,\cdots) \cdot P(s\_1,s\_2,s\_3,\cdots)$

#### 马尔可夫链

马尔可夫提出假设，即随机过程中各个状态 $s\_t$ 的概率分布，只与它的前一个状态 $s\_{t-1}$ 有关，即 $P(s\_t|s\_1,s\_2,s\_3,\cdots,s\_{t-1})=P(s\_t|s\_{t-1})$ 。

#### 隐含马尔可夫模型

隐含马尔可夫模型是上述马尔可夫链的一个扩展：任一时刻 $t$ 的状态 $S\_t$ 是不可见的。所以观察者没法通过观察到一个状态序列 $s\_1,s\_2,s\_3,\cdots,s\_T$ 来推测转移概率等参数。但是，隐含马尔可夫模型在每个时刻 $t$ 会输出一个符号 $o\_t$，而且 $o\_t$ 和 $S\_t$ 相关且仅和 $S\_t$ 相关。这个被称为独立输出假设。

基于马尔可夫假设和独立输出假设，我们可以计算出某个特定的状态序列 $s\_1,s\_2,s\_3,\cdots$ 产生出输出符号 $o\_1,o\_2,o\_3,\cdots$ 的概率。

$P(s\_1,s\_2,s\_3,\cdots|o\_1,o\_2,o\_3,\cdots) = \displaystyle\prod_t{P(s\_t|s\_{t-1}) \cdot P(o\_t|s\_t)}$

#### 三个基本问题

1. 给定一个模型，如何计算某个特定的输出序列的概率；
   Forward-Backward算法
2. 给定一个模型和某个特定的输出序列，如何找到最可能产生这个输出的状态序列；
   维特比算法
3. 给定足够量的观察数据，如何估计隐含马尔可夫模型的参数。
   模型的训练

#### 模型的训练

解决问题：给定足够量的观察数据，如何估计隐含马尔可夫模型的参数。

从前一个状态 $s\_{t-1}$ 进入当前状态 $s\_t$ 的概率P(s\_t|s\_{t-1})，称为转移概率（Transition Probability）；每个状态 $s\_t$ 产生相应输出符号 $o\_t$ 的概率 $P(o\_t|s\_t)$ ，称为生成概率（Generation Probability）。这些概率被称为隐含马尔可夫模型的参数。

有监督的训练方法（Supervised Traning）：

知道经过状态 $s\_t$ 的次数 $\\#(s\_t)$，以及每次经过这个状态时，分别产生的输出 $o\_t$ 是什么，而且分别有多少次 $\\#(o\_t, s\_t)$：

生成概率 $P(o\_t|s\_t) \approx \frac{#(o\_t, s\_t)}{#(s\_t)}

而转移概率可以依照统计语言模型的训练方法。

无监督的训练方法：

主要使用的是鲍姆-韦尔奇算法（Baum-Welch Algorithm）：

首先找到一组能够产生输出序列 $O$ 的模型参数，称为 $M\_{\Theta0}$ 。假定解决了第一个和第二个问题，不但可以算出这个模型产生 $O$ 的概率 $P(O|M\_{\Theta0})$，而且能够找到这个模型产生 $O$ 的所有可能路径以及这些路径的概率，可以将它们看做是“标注的训练数据”，根据之前的公式可以计算出新的模型参数 $\Theta\_1$ ，从 $M\_{\Theta0}$ 到 M\_{\Theta1} 的过程称为一次迭代。

可以证明 $P(O|M\_{\Theta1}) > P(O|M\_{\Theta0})$

按照上述流程不断找下去，直到模型的质量没有明显提高为止。


