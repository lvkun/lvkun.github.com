# 读书笔记 《数学之美》（二）

作者：吴军

## 如何确定网页和查询的相关性

根据网页的长度，对关键词的次数进行归一化，也就是用关键词的次数除以网页的总字数。称为“关键词的频率”，或者是“单文本词频”（Term Frequency）。

度量网页和查询的相关性，可以直接使用各个关键词在网页中出现的总词频。如果一个查询包含 $N$ 个关键词 $w\_1,w\_2,\cdots,w\_N$，它们在一个特定网页中的词频分别是：$TF\_1,TF\_2,\cdots,TF\_N$，那么这个查询和该网页的相关性就是：

$TF\_1+TF\_2+\cdots+TF\_N$

停止词（Stop Word）如“的”，在度量相关性时不应考虑它们的频率。

需要对汉语中的每一个词给一个权重，满足下面两个条件：

1. 一个词预测主题的能力越强，权重越大。
2. 停止词的权重为零。

概括地讲，假定一个关键词 $w$ 在 $D\_w$ 个网页中出现过，那么 $D\_w$ 越大， $w$ 的权重越小，反之亦然。在信息检索中，使用最多的权重是“逆文本频率指数”（Inverse Document Frequence， 缩写为IDF），公式为 $log(\frac{D}{D\_w})$ 。利用IDF，相关性计算公式为：

$TF\_1 \cdot IDF\_1 + TF\_2 \cdot IDF\_2 + \cdots + TF\_N \cdot IDF\_N$

## 有限状态机

有限状态机是一个特殊的有向图，它包括一些状态节点和连接这些状态的有向弧。每一个有限状态机都有一个开始状态和一个终止状态，以及若干中间状态。每一条弧上带有从一个状态进入下一个状态的条件。为了实现模糊匹配，并给出一个字串为正确地址的可能性，提出了基于概率的有限状态机。这种基于概率的有限状态机和离散的马尔可夫链基本上等效。

### 有限状态传感器

有限状态机严格的数学模型：

定义：有限状态机是一个五元组 $(\Sigma, S, s\_0, \delta, f)$

$\Sigma$ 是输入符号的集合，

$S$ 是一个非空的有限状态集合，

$s\_0$是 $S$ 中的一个特殊状态，起始状态，

$\delta$ 是一个从空间 $S \times \Sigma$ 到 $\S$ 的映射函数，即 $\delta : S \times \Sigma \rightarrow S$ ，

$f$ 是 $S$ 中另外一个特殊状态，终止状态。

在语音识别和加权的有限状态传感器（Weighted Finite State Transducer，简称WFST），特殊性在于，有限状态机中的每个状态由输入和输出符号定义。

WFST中每一条路径就是一个候选的句子，其中概率最大的那条路径就是这个句子的识别结果。

## 余弦定理和新闻的分类

### 新闻的特征向量

对于一篇新闻中的所有实词，计算出它们的TF-IDF值。把这些值按照对应的实词在词汇表的位置依次排列，就得到新闻的特征向量（Feature Vector）。

### 向量距离的度量

新闻 $X$ 和 $Y$ 对应的向量分别是：

$x\_1,x\_2,\cdots,x\_n$ 和 $y\_1,y\_2,\cdots,y\_n$ ，夹角的余弦为：

$cos\Theta = \frac{x\_1y\_1 + x\_2y\_2 + \cdots + x\_ny\_n}{\sqrt{x\_1^2+x\_2^2+\cdots+x\_n^2}\sqrt{y\_1^2+y\_2^2+\cdots+y\_n^2}}$

夹角在0度到90度之间。当两条新闻向量夹角的余弦接近于1时，两条新闻相似，从而可以归为一类。

自底向上不断合并的办法：

1. 计算所有新闻之间两两的余弦相似性，把相似性大于一个阈值的新闻合并为一个小类（Subclass）。这样 $N$ 篇新闻就被合并成 $N\_1$ 个小类，当然 $N\_1 < N$ 。
2. 把每个小类中所有的新闻作为一个整体，计算小类的特征向量，再计算小类之间两两的余弦相似性，然后合并成大一点的小类，假如有 $N\_2$ 个，当然 $N\_2 < N\_2$ 。

这样不断做下去，类别越来越少，而每个类越来越大。当某一类太大时，这一类里一些新闻之间的相似性就很小了，这时候就业停止迭代过程。

### 计算向量余弦的技巧

1. 分母部分（向量的长度）不需要重新计算
2. 两个向量内积的时候，只需考虑向量中的非零元素即可。
3. 删除虚词。

位置的加权：要对标题和重要位置的词进行额外的加权，以提高文本分类的准确性。

## 奇异值分解

用一个大矩阵 $A$ 来描述文章和词的关联性，每一行对应一篇文章，每一列对应一个词，其中第 $i$ 行，第 $j$ 列的元素 $a\_{ij}$ ，是字典中第 $j$ 个词在第 $i$ 篇文章中出现的加权词频（TF-IDF）。

奇异值分解：将矩阵 $A$ 分解成三个小矩阵相乘：

$A\_{MN} = X\_{MM} \times B\_{MN} \times Y\_{NN}$

其中 $X$ 是一个酉矩阵（Unitary Matrix），$Y$ 则是一个酉矩阵的共轭矩阵。而 $B$ 是一个对角阵。

矩阵 $X$ 是对词进行分类的结果，每一行表示一个词，每一列表示一个语义相近的词类，或者简称为语义类。这一行的每个非零元素表示这个词在每个语义类中的重要性（或者是相关性），数值越大越相关。

矩阵 $Y$ 是对文本的分类结果，每一列对应一个文本，每一行对应一个主题，这一列中的每个元素表示这篇文本在不同主题中的相关性。

矩阵 $B$ 表示词的类和文章的类之间的相似性。

奇异值分解的优点是能较快地得到结果，因为不需要一次次地迭代。但是用这种方法得到的分类结果略显粗糙，适合处理超大规模文本的粗分类。在实际工作中，可以先进行奇异值分解，得到粗分类结果，再利用计算向量余弦的方法，在粗分类结果的基础上，进行几次迭代，得到比较精确的结果。

## 最大熵模型

需要对一个随机事件的概率分布进行预测时，我们的预测应该满足全部已知的条件，而对未知的情况不要做任何主观假设。在这种情况下，概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们称这种模型叫“最大熵模型”。

希萨（I. Csiszar）证明，对任何一组不自相矛盾的信息，这个最大熵模型不仅存在，而且是唯一的。

### 最大熵模型的训练

假定搜索的排序需要考虑20种特征， $\{x\_1,x\_2,\cdots,x\_{20}\}$ ，需要排序的网页是 $d$ ，那么即使这些特征互相独立，对应的最大熵模型：

$P(d|x\_1,x\_2,\cdots,x\_{20}) = \frac{1}{Z(x\_1,x\_2,\cdots,x\_{20})}e^{\lambda\_1(x\_1,d)+\lambda\_2(x\_2,d)+\cdots+\lambda\_{20}(x\_{20},d)}$

其中归一化因子：

$Z(x\_1,x\_2,\cdots,x\_{20}) = \displaystyle\sum\_d{e^{\lambda\_1(x\_1,d)+\lambda\_2(x\_2,d)+\cdots+\lambda\_{20}(x\_{20},d)}}$

通用迭代算法：

1. 假定第零次迭代的初始模型为等概率的均匀分布；
2. 用第 $N$ 次迭代的模型来估算每种信息特征在训练数据中分布。如果超过实际的，就把相应的模型参数变小。否则，将它们变大。
3. 重复步骤 2 直到收敛。